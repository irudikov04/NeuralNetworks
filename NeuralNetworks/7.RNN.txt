import numpy as np

class SimpleRNN:
    def __init__(self, input_size, hidden_size, output_size):

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01  # Веса для скрытого слоя
        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01  # Веса для входа
        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01  # Веса для выхода

        self.b_h = np.zeros((1, hidden_size))  # Смещение для скрытого слоя
        self.b_y = np.zeros((1, output_size))  # Смещение для выходного слоя

    def forward(self, X):
     
        batch_size, seq_length, input_size = X.shape
        self.hidden_states = np.zeros((batch_size, seq_length, self.hidden_size))
        self.outputs = np.zeros((batch_size, seq_length, self.output_size))

        h_t = np.zeros((batch_size, self.hidden_size))

        for t in range(seq_length):
            x_t = X[:, t, :]
            h_t = np.tanh(np.dot(x_t, self.W_xh) + np.dot(h_t, self.W_hh) + self.b_h)
            self.hidden_states[:, t, :] = h_t
            y_t = np.dot(h_t, self.W_hy) + self.b_y
            self.outputs[:, t, :] = y_t

        return self.outputs, self.hidden_states

    def backward(self, X, y_true, learning_rate=0.01):

        batch_size, seq_length, input_size = X.shape
        _, _, output_size = y_true.shape

        dW_xh = np.zeros_like(self.W_xh)
        dW_hh = np.zeros_like(self.W_hh)
        dW_hy = np.zeros_like(self.W_hy)
        db_h = np.zeros_like(self.b_h)
        db_y = np.zeros_like(self.b_y)

        dh_next = np.zeros((batch_size, self.hidden_size))

        for t in reversed(range(seq_length)):

            y_pred = self.outputs[:, t, :]
            dy = y_pred - y_true[:, t, :]

            dW_hy += np.dot(self.hidden_states[:, t, :].T, dy)
            db_y += np.sum(dy, axis=0, keepdims=True)

            dh = np.dot(dy, self.W_hy.T) + dh_next

            dh_raw = (1 - self.hidden_states[:, t, :] ** 2) * dh
            db_h += np.sum(dh_raw, axis=0, keepdims=True)
            dW_xh += np.dot(X[:, t, :].T, dh_raw)
            dW_hh += np.dot(self.hidden_states[:, t - 1, :].T, dh_raw)

            dh_next = np.dot(dh_raw, self.W_hh.T)

        self.W_xh -= learning_rate * dW_xh
        self.W_hh -= learning_rate * dW_hh
        self.W_hy -= learning_rate * dW_hy
        self.b_h -= learning_rate * db_h
        self.b_y -= learning_rate * db_y

    def train(self, X, y_true, epochs=100, learning_rate=0.01):

        for epoch in range(epochs):

            outputs, _ = self.forward(X)

            self.backward(X, y_true, learning_rate)

            loss = np.mean((outputs - y_true) ** 2)
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss}")

if __name__ == "__main__":

    X = np.array([[[0.1], [0.2], [0.3], [0.4], [0.5]],
                  [[0.2], [0.3], [0.4], [0.5], [0.6]],
                  [[0.3], [0.4], [0.5], [0.6], [0.7]],
                  [[0.4], [0.5], [0.6], [0.7], [0.8]]])

    y_true = np.array([[[0.6], [0.6], [0.6], [0.6], [0.6]],
                       [[0.7], [0.7], [0.7], [0.7], [0.7]],
                       [[0.8], [0.8], [0.8], [0.8], [0.8]],
                       [[0.9], [0.9], [0.9], [0.9], [0.9]]])

    rnn = SimpleRNN(input_size=1, hidden_size=10, output_size=1)
    rnn.train(X, y_true, epochs=100, learning_rate=0.01)

    test_input = np.array([[[0.5], [0.6], [0.7], [0.8], [0.9]]])
    prediction, _ = rnn.forward(test_input)
    print("Прогноз:", prediction[:, -1, :])
