import numpy as np

class SimpleRNN:
    def __init__(self, input_size, hidden_size, output_size):
        """
        Инициализация RNN.
        :param input_size: Размерность входных данных (количество признаков).
        :param hidden_size: Размерность скрытого слоя.
        :param output_size: Размерность выходного слоя.
        """
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Инициализация весов случайными значениями
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01  # Веса для скрытого слоя
        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01  # Веса для входа
        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01  # Веса для выхода

        # Инициализация смещений
        self.b_h = np.zeros((1, hidden_size))  # Смещение для скрытого слоя
        self.b_y = np.zeros((1, output_size))  # Смещение для выходного слоя

    def forward(self, X):
        """
        Прямой проход через RNN.
        :param X: Входные данные (количество_образцов, длина_последовательности, размерность_входа).
        :return: Выходные данные и скрытое состояние.
        """
        batch_size, seq_length, input_size = X.shape
        self.hidden_states = np.zeros((batch_size, seq_length, self.hidden_size))
        self.outputs = np.zeros((batch_size, seq_length, self.output_size))

        # Инициализация скрытого состояния
        h_t = np.zeros((batch_size, self.hidden_size))

        for t in range(seq_length):
            # Вычисление скрытого состояния на текущем временном шаге
            x_t = X[:, t, :]  # Текущий вход
            h_t = np.tanh(np.dot(x_t, self.W_xh) + np.dot(h_t, self.W_hh) + self.b_h)
            self.hidden_states[:, t, :] = h_t

            # Вычисление выхода на текущем временном шаге
            y_t = np.dot(h_t, self.W_hy) + self.b_y
            self.outputs[:, t, :] = y_t

        return self.outputs, self.hidden_states

    def backward(self, X, y_true, learning_rate=0.01):
        """
        Обратный проход через RNN.
        :param X: Входные данные.
        :param y_true: Истинные значения.
        :param learning_rate: Скорость обучения.
        """
        batch_size, seq_length, input_size = X.shape
        _, _, output_size = y_true.shape

        # Инициализация градиентов
        dW_xh = np.zeros_like(self.W_xh)
        dW_hh = np.zeros_like(self.W_hh)
        dW_hy = np.zeros_like(self.W_hy)
        db_h = np.zeros_like(self.b_h)
        db_y = np.zeros_like(self.b_y)

        # Инициализация градиента скрытого состояния
        dh_next = np.zeros((batch_size, self.hidden_size))

        # Обратный проход по временным шагам
        for t in reversed(range(seq_length)):
            # Вычисление ошибки на выходе
            y_pred = self.outputs[:, t, :]
            dy = y_pred - y_true[:, t, :]

            # Градиенты для выходного слоя
            dW_hy += np.dot(self.hidden_states[:, t, :].T, dy)
            db_y += np.sum(dy, axis=0, keepdims=True)

            # Градиент скрытого состояния
            dh = np.dot(dy, self.W_hy.T) + dh_next

            # Градиенты для скрытого слоя
            dh_raw = (1 - self.hidden_states[:, t, :] ** 2) * dh
            db_h += np.sum(dh_raw, axis=0, keepdims=True)
            dW_xh += np.dot(X[:, t, :].T, dh_raw)
            dW_hh += np.dot(self.hidden_states[:, t - 1, :].T, dh_raw)

            # Передача градиента на следующий временной шаг
            dh_next = np.dot(dh_raw, self.W_hh.T)

        # Обновление весов и смещений
        self.W_xh -= learning_rate * dW_xh
        self.W_hh -= learning_rate * dW_hh
        self.W_hy -= learning_rate * dW_hy
        self.b_h -= learning_rate * db_h
        self.b_y -= learning_rate * db_y

    def train(self, X, y_true, epochs=100, learning_rate=0.01):
        """
        Обучение RNN.
        :param X: Входные данные.
        :param y_true: Истинные значения.
        :param epochs: Количество эпох.
        :param learning_rate: Скорость обучения.
        """
        for epoch in range(epochs):
            # Прямой проход
            outputs, _ = self.forward(X)

            # Обратный проход
            self.backward(X, y_true, learning_rate)

            # Вывод потерь (MSE)
            loss = np.mean((outputs - y_true) ** 2)
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss}")

# Пример использования
if __name__ == "__main__":
    # Пример данных: временные ряды
    X = np.array([[[0.1], [0.2], [0.3], [0.4], [0.5]],
                  [[0.2], [0.3], [0.4], [0.5], [0.6]],
                  [[0.3], [0.4], [0.5], [0.6], [0.7]],
                  [[0.4], [0.5], [0.6], [0.7], [0.8]]])

    # Целевые значения: последний элемент в каждой последовательности
    y_true = np.array([[[0.6], [0.6], [0.6], [0.6], [0.6]],
                       [[0.7], [0.7], [0.7], [0.7], [0.7]],
                       [[0.8], [0.8], [0.8], [0.8], [0.8]],
                       [[0.9], [0.9], [0.9], [0.9], [0.9]]])

    # Создание и обучение RNN
    rnn = SimpleRNN(input_size=1, hidden_size=10, output_size=1)
    rnn.train(X, y_true, epochs=100, learning_rate=0.01)

    # Прогнозирование
    test_input = np.array([[[0.5], [0.6], [0.7], [0.8], [0.9]]])
    prediction, _ = rnn.forward(test_input)
    print("Прогноз:", prediction[:, -1, :])